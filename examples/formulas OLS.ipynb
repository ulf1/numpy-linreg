{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is OLS? How to estimate model parameters?\n",
    "Let $y \\in \\mathbb{R}^{n \\times 1}$ the dependent variable, \n",
    "$X \\in \\mathbb{R}^{n \\times m}$ the indendent variables (including the intercept), \n",
    "and $\\beta \\in \\mathbb{R}^{m \\times 1}$ the regression coefficients.\n",
    "\n",
    "The residuals or errors $\\epsilon \\in \\mathbb{R}^{n \\times 1}$ are the difference between $y$ and the prediction $X\\beta$ \n",
    "\n",
    "$$\n",
    "\\epsilon = y - X \\beta\n",
    "$$\n",
    "\n",
    "and the **Mean Squared Error** is\n",
    "\n",
    "$$\n",
    "MSE(\\beta) = \\frac{1}{n} \\epsilon^T \\epsilon \n",
    "$$\n",
    "\n",
    "The **gradient** of the MSE is\n",
    "\n",
    "$$\n",
    "\\nabla \\, MSE(\\beta) = \\frac{2}{n} (X^T X\\beta - X^T y)\n",
    "$$\n",
    "\n",
    "The gradient would be $\\nabla MSE(\\hat{\\beta}) = 0$ for the **optimum** solution $\\hat{\\beta}$\n",
    "\n",
    "$$\n",
    "0 = X^T X\\hat{\\beta} - X^T y \\\\\n",
    "\\Leftrightarrow \\hat{\\beta} = (X^T X)^{-1} - (X^T y)\n",
    "$$\n",
    "\n",
    "The Ordinary Least Square (OLS) method is basically a) setting the gradient of the MSE to zero, and b) solving after $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LU decomposition\n",
    "Let $A=X^T X$, $x=\\beta$ and $b=X^T y$.\n",
    "At its core `numpy.linalg.solve` tries to solve $Ax=b$ \n",
    "\n",
    "$$\n",
    "\\min_x Ax - b\n",
    "$$\n",
    "\n",
    "wheras LU decomposition is applied to $A = LU = PA$ at some point in the program.\n",
    "\n",
    "The python code is \n",
    "\n",
    "```\n",
    "beta = np.linalg.solve(np.dot(X.T, X), np.dot(X.T, y))\n",
    "```\n",
    "\n",
    "The scipy is more explicit but slower because it calls LAPACK twice (`lu_solve` and `lu_factor`) and thus has more overhead.\n",
    "\n",
    "```\n",
    "beta = scipy.linalg.lu_solve(scipy.linalg.lu_factor(np.dot(X.T,X)), np.dot(X.T,y))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo-Inverse\n",
    "The `p` stands for **p**seudoinverse or Moore-**P**enrose inverse.\n",
    "(Just use the mnemonic that fits your brain.)\n",
    "\n",
    "The pseudoinverse is the inverse based on Singular Value Decomposition but with the correction for ill-conditioned matrices.\n",
    "\n",
    "The task is to estimate\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (X^T X)^{-1} - (X^T y)\n",
    "$$\n",
    "\n",
    "whereas the inverse $A^{-1} = (X^T X)^{-1}$ is required.\n",
    "\n",
    "1. Conduct SVD: $A = U S V^T$\n",
    "2. Compute the inverse: $A^{-1} = V S^{-1} U^T$\n",
    "\n",
    "If $A A^T$ is an ill-conditioned or resp. singular matrix then some of $S$'s diagonal elements $diag(s_1, s_2, .., s_n)$ are close to zero and the division-by-zero problems arises. \n",
    "The pseudoinverse approach just sets $\\frac{1}{s_i}=0$ if a element's value is below a certain threshold $s_i<tol$.\n",
    "\n",
    "**When to apply pinv?**\n",
    "* not at all because you are not supposed to conduct Linear Regression on ill-conditioned matrices (e.g. multicollinarity)\n",
    "* there is nothing you can do about $X$ (e.g. your customer want exactly these variables included, it's the best data you got, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QR Decomposition\n",
    "The regressions coefficients \n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (X^T X)^{-1} - (X^T y)\n",
    "$$\n",
    "\n",
    "will be estimated by compute the inverse matrix $A^{-1} = (X^T X)^{-1}$ with QR Decomposition $A = Q R$\n",
    "\n",
    "$$\n",
    "A^{-1} = R^{-1} Q^T\n",
    "$$\n",
    "\n",
    "The advantage is that $R$ is a triangular matrix that can be easily inverted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition (SVD)\n",
    "The regressions coefficients \n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (X^T X)^{-1} - (X^T y)\n",
    "$$\n",
    "\n",
    "will be estimated by compute the inverse matrix $(X^T X)^{-1}$ with Singular Value Decomposition (SVD).\n",
    "\n",
    "SVD decompose a matrix $A$ into \n",
    "\n",
    "$$\n",
    "A = U S V^T\n",
    "$$\n",
    "\n",
    "with $U U^T = I$ and $V V^T = I$ and $S$ a diagonal matrix.\n",
    "\n",
    "**Numpy lstsq**\n",
    "The third implementation applies Numpy's [lstsq](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.lstsq.html) is based on LAPACK's [_gelsd](http://www.netlib.org/lapack/explore-html/d7/d3b/group__double_g_esolve_ga94bd4a63a6dacf523e25ff617719f752.html) what applies SVD to solve problems of type\n",
    "\n",
    "$$\n",
    "\\min_x || b - A x ||_2\n",
    "$$\n",
    "\n",
    "For a Linear Regression problem the variables are $A=X^T X$, $x=\\beta$ and $b=X^T y$.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
